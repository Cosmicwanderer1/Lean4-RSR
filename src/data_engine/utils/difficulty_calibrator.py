"""
è‡ªé€‚åº”éš¾åº¦é˜ˆå€¼æ ¡å‡†å™¨

åŸºäºå†å²æ•°æ®åŠ¨æ€è°ƒæ•´éš¾åº¦åˆ¤æ–­çš„é˜ˆå€¼ï¼Œæ”¯æŒï¼š
1. è®°å½•é¢„æµ‹ç»“æœå’Œå®é™…ç»“æœ
2. æ ¹æ®å†å²æ•°æ®è‡ªåŠ¨æ ¡å‡†é˜ˆå€¼
3. è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import json
import os
from dataclasses import dataclass, asdict
from typing import List, Dict, Optional, Tuple
from pathlib import Path


@dataclass
class CalibrationRecord:
    """å•æ¡æ ¡å‡†è®°å½•"""
    theorem_name: str
    adjusted_score: float
    predicted_difficulty: str
    actual_difficulty: str  # é€šè¿‡ç¼–è¯‘éªŒè¯æˆ–äººå·¥æ ‡æ³¨
    forward_score: int
    backward_score: int
    timestamp: str = ""


class DifficultyCalibrator:
    """
    åŸºäºå†å²æ•°æ®åŠ¨æ€æ ¡å‡†éš¾åº¦é˜ˆå€¼

    ä½¿ç”¨æ–¹æ³•ï¼š
    1. åˆå§‹åŒ–æ—¶åŠ è½½å†å²è®°å½•
    2. è°ƒç”¨ record() è®°å½•æ¯æ¬¡é¢„æµ‹
    3. å®šæœŸè°ƒç”¨ calibrate() æ›´æ–°é˜ˆå€¼
    4. ä½¿ç”¨ get_thresholds() è·å–å½“å‰é˜ˆå€¼
    """

    def __init__(self, history_file: str = None):
        """
        åˆå§‹åŒ–æ ¡å‡†å™¨

        Args:
            history_file: å†å²è®°å½•æ–‡ä»¶è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼‰
        """
        # é»˜è®¤é˜ˆå€¼
        self.easy_threshold = 0.75
        self.medium_threshold = 0.50

        # å†å²è®°å½•
        self.history: List[CalibrationRecord] = []
        self.history_file = history_file

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_records': 0,
            'correct_predictions': 0,
            'last_calibration': None
        }

        # åŠ è½½å†å²æ•°æ®
        if history_file and os.path.exists(history_file):
            self._load_history(history_file)
            if len(self.history) >= 50:
                self.calibrate()

    def _load_history(self, filepath: str):
        """åŠ è½½å†å²è®°å½•"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        self.history.append(CalibrationRecord(**data))
            print(f"[Calibrator] Loaded {len(self.history)} historical records")
        except Exception as e:
            print(f"[Calibrator] Error loading history: {e}")

    def _save_history(self):
        """ä¿å­˜å†å²è®°å½•"""
        if not self.history_file:
            return

        try:
            os.makedirs(os.path.dirname(self.history_file), exist_ok=True)
            with open(self.history_file, 'w', encoding='utf-8') as f:
                for record in self.history:
                    f.write(json.dumps(asdict(record), ensure_ascii=False) + '\n')
        except Exception as e:
            print(f"[Calibrator] Error saving history: {e}")

    def record(
        self,
        theorem_name: str,
        adjusted_score: float,
        predicted_difficulty: str,
        actual_difficulty: str,
        forward_score: int = 0,
        backward_score: int = 0
    ):
        """
        è®°å½•ä¸€æ¬¡é¢„æµ‹ç»“æœ

        Args:
            theorem_name: å®šç†åç§°
            adjusted_score: è°ƒæ•´åçš„åˆ†æ•° (0-1)
            predicted_difficulty: é¢„æµ‹çš„éš¾åº¦ (Easy/Medium/Hard)
            actual_difficulty: å®é™…éš¾åº¦ï¼ˆé€šè¿‡ç¼–è¯‘éªŒè¯æˆ–æ ‡æ³¨ï¼‰
            forward_score: Forward è¯„åˆ†
            backward_score: Backward è¯„åˆ†
        """
        from datetime import datetime

        record = CalibrationRecord(
            theorem_name=theorem_name,
            adjusted_score=adjusted_score,
            predicted_difficulty=predicted_difficulty.lower().replace(" (inferred)", "").replace(" (original)", ""),
            actual_difficulty=actual_difficulty.lower(),
            forward_score=forward_score,
            backward_score=backward_score,
            timestamp=datetime.now().isoformat()
        )

        self.history.append(record)
        self.stats['total_records'] += 1

        if record.predicted_difficulty == record.actual_difficulty:
            self.stats['correct_predictions'] += 1

        # è‡ªåŠ¨ä¿å­˜
        if self.history_file and len(self.history) % 10 == 0:
            self._save_history()

        # å®šæœŸæ ¡å‡†ï¼ˆæ¯ 50 æ¡è®°å½•ï¼‰
        if len(self.history) % 50 == 0 and len(self.history) >= 50:
            self.calibrate()

    def calibrate(self) -> Tuple[float, float]:
        """
        æ ¹æ®å†å²æ•°æ®æ ¡å‡†é˜ˆå€¼

        ä½¿ç”¨åˆ†ä½æ•°æ–¹æ³•ï¼š
        - Easy é˜ˆå€¼ = Easy æ ·æœ¬åˆ†æ•°çš„ 25% åˆ†ä½æ•°
        - Medium é˜ˆå€¼ = Hard æ ·æœ¬åˆ†æ•°çš„ 75% åˆ†ä½æ•°

        Returns:
            (easy_threshold, medium_threshold)
        """
        if len(self.history) < 50:
            print(f"[Calibrator] Not enough data ({len(self.history)}/50), using defaults")
            return self.easy_threshold, self.medium_threshold

        # æŒ‰å®é™…éš¾åº¦åˆ†ç»„
        easy_scores = [r.adjusted_score for r in self.history if r.actual_difficulty == 'easy']
        medium_scores = [r.adjusted_score for r in self.history if r.actual_difficulty == 'medium']
        hard_scores = [r.adjusted_score for r in self.history if r.actual_difficulty == 'hard']

        # è®¡ç®—æ–°é˜ˆå€¼
        if easy_scores and hard_scores:
            # ä½¿ç”¨åˆ†ä½æ•°é¿å…ç¦»ç¾¤å€¼å½±å“
            easy_scores_sorted = sorted(easy_scores)
            hard_scores_sorted = sorted(hard_scores)

            # Easy é˜ˆå€¼ï¼šEasy æ ·æœ¬çš„ä¸‹ 25% åˆ†ä½æ•°ï¼ˆä¿å®ˆï¼‰
            easy_idx = max(0, int(len(easy_scores_sorted) * 0.25) - 1)
            new_easy_threshold = easy_scores_sorted[easy_idx]

            # Medium é˜ˆå€¼ï¼šHard æ ·æœ¬çš„ä¸Š 75% åˆ†ä½æ•°
            hard_idx = min(len(hard_scores_sorted) - 1, int(len(hard_scores_sorted) * 0.75))
            new_medium_threshold = hard_scores_sorted[hard_idx]

            # ç¡®ä¿ easy > medium
            if new_easy_threshold > new_medium_threshold + 0.1:
                old_easy, old_medium = self.easy_threshold, self.medium_threshold
                self.easy_threshold = new_easy_threshold
                self.medium_threshold = new_medium_threshold

                print(f"[Calibrator] Thresholds updated:")
                print(f"  Easy:   {old_easy:.3f} -> {self.easy_threshold:.3f}")
                print(f"  Medium: {old_medium:.3f} -> {self.medium_threshold:.3f}")
            else:
                print(f"[Calibrator] New thresholds invalid (easy={new_easy_threshold:.3f}, "
                      f"medium={new_medium_threshold:.3f}), keeping defaults")

        from datetime import datetime
        self.stats['last_calibration'] = datetime.now().isoformat()

        return self.easy_threshold, self.medium_threshold

    def get_thresholds(self) -> Tuple[float, float]:
        """è·å–å½“å‰é˜ˆå€¼"""
        return self.easy_threshold, self.medium_threshold

    def get_accuracy(self) -> Dict[str, float]:
        """
        è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡

        Returns:
            {
                'overall': æ€»ä½“å‡†ç¡®ç‡,
                'easy': Easy å‡†ç¡®ç‡,
                'medium': Medium å‡†ç¡®ç‡,
                'hard': Hard å‡†ç¡®ç‡
            }
        """
        if not self.history:
            return {'overall': 0, 'easy': 0, 'medium': 0, 'hard': 0}

        results = {'easy': [0, 0], 'medium': [0, 0], 'hard': [0, 0]}  # [correct, total]

        for record in self.history:
            actual = record.actual_difficulty
            if actual in results:
                results[actual][1] += 1
                if record.predicted_difficulty == actual:
                    results[actual][0] += 1

        return {
            'overall': self.stats['correct_predictions'] / max(self.stats['total_records'], 1),
            'easy': results['easy'][0] / max(results['easy'][1], 1),
            'medium': results['medium'][0] / max(results['medium'][1], 1),
            'hard': results['hard'][0] / max(results['hard'][1], 1)
        }

    def generate_report(self) -> str:
        """ç”Ÿæˆæ ¡å‡†æŠ¥å‘Š"""
        accuracy = self.get_accuracy()

        report = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              DIFFICULTY CALIBRATOR REPORT                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“Š Current Thresholds:
   Easy   >= {self.easy_threshold:.3f}
   Medium >= {self.medium_threshold:.3f}
   Hard   <  {self.medium_threshold:.3f}

ğŸ“ˆ Statistics:
   Total Records:       {self.stats['total_records']}
   Correct Predictions: {self.stats['correct_predictions']}
   Last Calibration:    {self.stats['last_calibration'] or 'Never'}

ğŸ¯ Accuracy:
   Overall: {accuracy['overall']:.1%}
   Easy:    {accuracy['easy']:.1%}
   Medium:  {accuracy['medium']:.1%}
   Hard:    {accuracy['hard']:.1%}

ğŸ“‰ Score Distribution:
"""
        # æ·»åŠ åˆ†æ•°åˆ†å¸ƒ
        if self.history:
            easy_scores = [r.adjusted_score for r in self.history if r.actual_difficulty == 'easy']
            medium_scores = [r.adjusted_score for r in self.history if r.actual_difficulty == 'medium']
            hard_scores = [r.adjusted_score for r in self.history if r.actual_difficulty == 'hard']

            if easy_scores:
                report += f"   Easy:   min={min(easy_scores):.3f}, max={max(easy_scores):.3f}, avg={sum(easy_scores)/len(easy_scores):.3f}\n"
            if medium_scores:
                report += f"   Medium: min={min(medium_scores):.3f}, max={max(medium_scores):.3f}, avg={sum(medium_scores)/len(medium_scores):.3f}\n"
            if hard_scores:
                report += f"   Hard:   min={min(hard_scores):.3f}, max={max(hard_scores):.3f}, avg={sum(hard_scores)/len(hard_scores):.3f}\n"

        return report


# å…¨å±€å•ä¾‹ï¼ˆå¯é€‰ï¼‰
_global_calibrator: Optional[DifficultyCalibrator] = None


def get_calibrator(history_file: str = None) -> DifficultyCalibrator:
    """è·å–å…¨å±€æ ¡å‡†å™¨å®ä¾‹"""
    global _global_calibrator
    if _global_calibrator is None:
        _global_calibrator = DifficultyCalibrator(history_file)
    return _global_calibrator
