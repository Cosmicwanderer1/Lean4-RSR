import yaml
import torch
import os
from datasets import load_dataset
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
)
from trl import SFTTrainer
from src.common.rsr_prompts import format_rsr_input

def load_config():
    # ç¡®ä¿è¯»å–æ­£ç¡®çš„æ–‡ä»¶
    config_path = "configs/config.yaml"
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found at {config_path}")
    with open(config_path, "r", encoding='utf-8') as f:
        return yaml.safe_load(f)

def formatting_func(example):
    """
    å°†æ•°æ®è½¬æ¢ä¸º Qwen/ChatML æ ¼å¼
    """
    output_texts = []
    
    inputs = example['input']
    targets = example['target']
    
    if isinstance(inputs, str):
        inputs = [inputs]
        targets = [targets]
    
    for i in range(len(inputs)):
        if not inputs[i]:
            continue
            
        prompt = format_rsr_input(inputs[i])
        target_str = str(targets[i]) if targets[i] is not None else ""
        # åŠ ä¸Š EOS token ç¡®ä¿æ¨¡å‹çŸ¥é“ä»€ä¹ˆæ—¶å€™åœæ­¢
        text = prompt + target_str + "<|im_end|>"
        output_texts.append(text)
        
    return output_texts

def main():
    cfg = load_config()
    model_id = cfg["model"]["base_model_id"]
    output_dir = cfg["project"]["output_dir"]
    data_path = cfg["data"]["synthetic_path"]
    
    # 1. æ£€æµ‹ç¡¬ä»¶æ”¯æŒï¼Œä¼˜å…ˆä½¿ç”¨ BF16 (RTX 30/40ç³»åˆ—ä¸“ç”¨)
    # BF16 æ¯” FP16 è®­ç»ƒæ›´ç¨³å®šï¼Œä¸æ˜“æº¢å‡º
    use_bf16 = torch.cuda.is_bf16_supported()
    compute_dtype = torch.bfloat16 if use_bf16 else torch.float16
    
    print(f"ğŸš€ Starting training pipeline (Pro Mode)...")
    print(f"   Base Model: {model_id}")
    print(f"   Compute Dtype: {compute_dtype}")
    print(f"   Max Length: {cfg['data']['max_length']}")

    # 2. é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=cfg["training"]["use_4bit"],
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=compute_dtype, # è¿™é‡Œè”åŠ¨ä¿®æ”¹
        bnb_4bit_use_double_quant=True
    )

    # 3. åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        quantization_config=bnb_config, 
        device_map="auto",
        trust_remote_code=True,
        # æ˜¾å¼æŒ‡å®šæ³¨æ„åŠ›å®ç°ï¼ŒSDPA æ˜¯ PyTorch 2.0+ åŸç”ŸåŠ é€Ÿï¼Œçœæ˜¾å­˜
        attn_implementation="sdpa" 
    )

    # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing) - 7Bæ¨¡å‹è·‘é•¿æ–‡æœ¬å¿…å¼€ï¼
    model.gradient_checkpointing_enable()
    model.config.use_cache = False # è®­ç»ƒæ—¶å¿…é¡»å…³é—­ KV Cache
    
    model = prepare_model_for_kbit_training(model)
    
    # åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right" # SFT è®­ç»ƒé€šå¸¸å³ä¾§ padding

    # 4. LoRA é…ç½®
    # ä» config ä¸­åŠ¨æ€è¯»å– target_modulesï¼Œä¸å†ç¡¬ç¼–ç 
    target_modules = cfg["training"].get("lora_target_modules", ["q_proj", "v_proj"])
    
    peft_config = LoraConfig(
        r=cfg["training"]["lora_r"],
        lora_alpha=cfg["training"]["lora_alpha"],
        lora_dropout=cfg["training"].get("lora_dropout", 0.05),
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=target_modules, 
    )
    
    print(f"   LoRA Config: r={peft_config.r}, targets={peft_config.target_modules}")

    # 5. åŠ è½½æ•°æ®
    dataset = load_dataset("json", data_files=data_path, split="train")
    print(f"âœ… Loaded {len(dataset)} training samples.")

    # 6. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=cfg["training"]["batch_size"],
        gradient_accumulation_steps=cfg["training"]["grad_accumulation"],
        learning_rate=float(cfg["training"]["learning_rate"]),
        num_train_epochs=cfg["training"]["num_epochs"],
        
        # === ç²¾åº¦ä¸ä¼˜åŒ– ===
        bf16=use_bf16,        # RTX 4090 å¼€å¯ BF16
        fp16=not use_bf16,    # æ—§å¡å¼€å¯ FP16
        gradient_checkpointing=True, # ã€å…³é”®ã€‘å¿…é¡»å¼€å¯ï¼Œå¦åˆ™çˆ†æ˜¾å­˜
        optim="paged_adamw_32bit",   # ä½¿ç”¨åˆ†é¡µä¼˜åŒ–å™¨ï¼Œè¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜
        
        logging_steps=1,
        save_strategy="epoch",
        warmup_ratio=0.03,
        report_to=["tensorboard"], # å»æ‰ wandb é¿å…æ²¡é…ç½®æŠ¥é”™ï¼Œåªæœ‰ TensorBoard ä¹Ÿå¯ä»¥
        run_name=cfg["project"]["name"],
        remove_unused_columns=True, 
    )

    # 7. Trainer
    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        peft_config=peft_config,
        formatting_func=formatting_func,
        tokenizer=tokenizer,
        args=training_args,
        max_seq_length=cfg["data"]["max_length"],
        packing=False
    )

    print("ğŸ”¥ Starting training...")
    trainer.train()
    
    print(f"ğŸ’¾ Saving model to {output_dir}")
    trainer.model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print("ğŸ‰ Training Complete!")

if __name__ == "__main__":
    main()