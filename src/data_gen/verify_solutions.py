#!/usr/bin/env python3
"""
Lean 4 Proof Verifier - Optimized Version
é«˜æ•ˆå¹¶è¡ŒéªŒè¯Leanå®šç†è¯æ˜ï¼Œè‡ªåŠ¨ç­›é€‰ç”Ÿæˆé»„é‡‘æ•°æ®é›†
"""

import json
import os
import subprocess
import tempfile
import multiprocessing
import re
import hashlib
import argparse
import sys
import uuid
import logging
import time
import shutil
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any, Set
from tqdm import tqdm
from dataclasses import dataclass, asdict, field
import signal
import gc
import psutil
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from collections import defaultdict
import pickle
from enum import Enum
import traceback
import platform

# --- æ—¥å¿—é…ç½® ---
class ColoredFormatter(logging.Formatter):
    """å½©è‰²æ—¥å¿—æ ¼å¼åŒ–å™¨"""
    COLORS = {
        'DEBUG': '\033[36m',      # é’è‰²
        'INFO': '\033[32m',       # ç»¿è‰²
        'WARNING': '\033[33m',    # é»„è‰²
        'ERROR': '\033[31m',      # çº¢è‰²
        'CRITICAL': '\033[41m',   # çº¢åº•ç™½å­—
        'RESET': '\033[0m'
    }
    
    def format(self, record):
        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        # é¿å…ä¿®æ”¹åŸå§‹ record å¯¹è±¡ï¼Œé˜²æ­¢å½±å“å…¶ä»– handler
        record.levelname = f"{color}{record.levelname}{self.COLORS['RESET']}"
        record.msg = f"{color}{record.msg}{self.COLORS['RESET']}"
        return super().format(record)

def setup_logging(log_file: str = None):
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    # ç§»é™¤é»˜è®¤çš„ logger é…ç½®ï¼Œé˜²æ­¢é‡å¤æ‰“å°
    root_logger = logging.getLogger()
    if root_logger.handlers:
        root_logger.handlers.clear()
    root_logger.setLevel(logging.INFO)

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_formatter = ColoredFormatter(
        '%(asctime)s - %(levelname)s - %(processName)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    root_logger.addHandler(console_handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(processName)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(file_formatter)
        root_logger.addHandler(file_handler)
    
    return root_logger

logger = logging.getLogger(__name__)

# --- æšä¸¾å’Œå¸¸é‡ ---
class VerificationStatus(Enum):
    """éªŒè¯çŠ¶æ€æšä¸¾"""
    SUCCESS = "success"
    COMPILE_ERROR = "compile_error"
    TIMEOUT = "timeout"
    MEMORY_LIMIT = "memory_limit"
    INVALID_FORMAT = "invalid_format"
    CONTAINS_SORRY = "contains_sorry"
    SYSTEM_ERROR = "system_error"

# --- æ•°æ®ç±» ---
@dataclass
class VerificationResult:
    """éªŒè¯ç»“æœæ•°æ®ç±»"""
    task_id: str
    original_decl: str
    solution: str
    proof_only: str
    normalized_hash: str
    length: int
    is_complete_proof: bool
    verification_time: float
    status: VerificationStatus
    lean_version: Optional[str] = None
    memory_used_mb: Optional[float] = None
    stats: Optional[Dict] = None
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = asdict(self)
        result['status'] = self.status.value
        return result

@dataclass
class SystemStats:
    """ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    total_tasks: int = 0
    processed_tasks: int = 0
    successful_tasks: int = 0
    failed_tasks: int = 0
    total_memory_used_mb: float = 0.0
    total_verification_time: float = 0.0
    start_time: float = field(default_factory=time.time)
    
    def update_stats(self, result: Optional[VerificationResult] = None, 
                    failed: bool = False, memory_used: float = 0.0):
        """æ›´æ–°ç»Ÿè®¡ä¿¡æ¯"""
        self.processed_tasks += 1
        
        if result and result.status == VerificationStatus.SUCCESS:
            self.successful_tasks += 1
            self.total_verification_time += result.verification_time
        else:
            self.failed_tasks += 1
            
        self.total_memory_used_mb += memory_used
    
    def get_summary(self) -> Dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        elapsed = time.time() - self.start_time
        return {
            "total_tasks": self.total_tasks,
            "processed_tasks": self.processed_tasks,
            "successful_tasks": self.successful_tasks,
            "failed_tasks": self.failed_tasks,
            "success_rate": self.successful_tasks / max(1, self.processed_tasks),
            "avg_memory_mb": self.total_memory_used_mb / max(1, self.processed_tasks),
            "avg_time_per_task": self.total_verification_time / max(1, self.successful_tasks),
            "total_time_seconds": elapsed,
            "tasks_per_second": self.processed_tasks / max(1, elapsed)
        }

# --- é…ç½®ç®¡ç†å™¨ ---
class Config:
    """é…ç½®ç®¡ç†å™¨"""
    # è·¯å¾„é…ç½®
    DEFAULT_INPUT_FILE = "data/processed/solutions_shard_0.jsonl"
    DEFAULT_OUTPUT_FILE = "data/processed/verified_gold_data.jsonl"
    LEAN_GYM_PATH = os.path.abspath("lean_gym")
    CACHE_DIR = ".verification_cache"
    
    # éªŒè¯å‚æ•°
    TIMEOUT = 45
    TIMEOUT_LONG = 120  # é•¿è¯æ˜çš„è¶…æ—¶æ—¶é—´
    NUM_WORKERS = max(1, multiprocessing.cpu_count() - 1)
    MAX_MEMORY_PER_WORKER_MB = 4096  # 4GB
    MAX_TOTAL_MEMORY_MB = 32768  # 32GB æ€»é™åˆ¶
    
    # éªŒè¯é€‰é¡¹
    STRICT_BAD_PATTERNS = re.compile(r"(sorry|admit|axiom|undefined)", re.IGNORECASE)
    WARNING_PATTERNS = re.compile(r"warning:", re.IGNORECASE)
    HEADER = "import Mathlib\nopen Classical\n\n"
    
    # ç¼“å­˜é…ç½®
    CACHE_MAX_SIZE = 10000
    ENABLE_CACHE = True
    ENABLE_INCREMENTAL = True
    
    # ä¸´æ—¶ç›®å½•
    @staticmethod
    def get_temp_dir() -> str:
        """è·å–ä¸´æ—¶ç›®å½•"""
        # å°è¯•å¤šä¸ªå¯èƒ½çš„ä¸´æ—¶ç›®å½•ä½ç½®
        candidates = [
            "/root/autodl-fs/lean_verify_tmp",  # AutoDL
            "/data/lean_verify_tmp",  # é€šç”¨æ•°æ®ç›®å½•
            "/tmp/lean_verify",
            str(Path.home() / ".lean_verify_tmp"),
            os.path.join(tempfile.gettempdir(), "lean_verify")
        ]
        
        for candidate in candidates:
            try:
                path = Path(candidate)
                path.mkdir(parents=True, exist_ok=True)
                # æµ‹è¯•å†™å…¥æƒé™
                test_file = path / ".write_test"
                with open(test_file, 'w') as f:
                    f.write("test")
                test_file.unlink()
                return str(path.absolute())
            except (OSError, PermissionError):
                continue
        
        # å¦‚æœæ‰€æœ‰å€™é€‰ç›®å½•éƒ½å¤±è´¥ï¼Œä½¿ç”¨å½“å‰ç›®å½•
        fallback = Path("lean_verify_tmp")
        fallback.mkdir(exist_ok=True)
        return str(fallback.absolute())
    
    TEMP_DIR = get_temp_dir.__func__()
    
    @classmethod
    def update_from_args(cls, args):
        """æ ¹æ®å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®"""
        cls.LEAN_GYM_PATH = os.path.abspath(args.lean_gym_path)
        cls.TIMEOUT = args.timeout
        cls.NUM_WORKERS = args.num_workers
        cls.MAX_MEMORY_PER_WORKER_MB = args.max_memory_mb
        cls.ENABLE_CACHE = not args.disable_cache
        cls.ENABLE_INCREMENTAL = not args.disable_incremental

# --- å·¥å…·å‡½æ•° ---
class CodeNormalizer:
    """ä»£ç è§„èŒƒåŒ–å™¨"""
    
    @staticmethod
    def normalize_code(code: str) -> str:
        """
        è§„èŒƒåŒ–ä»£ç ç”¨äºå“ˆå¸Œè®¡ç®—
        - ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦
        - æ ‡å‡†åŒ–ç¼©è¿›
        - ç§»é™¤æ³¨é‡Š
        """
        # ç§»é™¤å•è¡Œæ³¨é‡Šå’Œå¤šè¡Œæ³¨é‡Š
        lines = []
        for line in code.split('\n'):
            # ç§»é™¤è¡Œå†…æ³¨é‡Š
            if '--' in line:
                line = line[:line.index('--')]
            lines.append(line.strip())
        
        code_no_comments = '\n'.join(lines)
        
        # ç§»é™¤å¤šè¡Œæ³¨é‡Š (ç®€åŒ–ç‰ˆæœ¬)
        code_no_comments = re.sub(r'/-[\s\S]*?-\/', '', code_no_comments)
        
        # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦å¹¶æ ‡å‡†åŒ–
        normalized = re.sub(r'\s+', ' ', code_no_comments).strip()
        return normalized
    
    @staticmethod
    def extract_code_from_markdown(text: str) -> str:
        """ä»Markdownä¸­æå–Leanä»£ç å—"""
        if not text:
            return ""
        
        # ç§»é™¤å¯èƒ½çš„HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # åŒ¹é…ä»£ç å—
        patterns = [
            r'```(?:lean)?\s*(.*?)```',  # ```lean ... ```
            r'```\s*(.*?)```',           # ``` ... ```
            r'`(.*?)`',                  # `...`
        ]
        
        code_blocks = []
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL)
            for match in matches:
                if match and len(match.strip()) > 10:  # æœ€å°é•¿åº¦é˜ˆå€¼
                    code_blocks.append(match.strip())
        
        if code_blocks:
            # é€‰æ‹©æœ€é•¿çš„ä»£ç å—
            return max(code_blocks, key=len)
        
        # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•æå–å¯èƒ½æ˜¯ä»£ç çš„è¡Œ
        lines = text.split('\n')
        code_lines = []
        
        for line in lines:
            line_stripped = line.strip()
            
            # å¯å‘å¼æ£€æµ‹ä»£ç è¡Œ
            is_code_line = (
                line_stripped.startswith('theorem') or
                line_stripped.startswith('lemma') or
                line_stripped.startswith('example') or
                line_stripped.startswith('def') or
                line_stripped.startswith('by ') or
                line_stripped.startswith('calc') or
                ':=' in line_stripped
            )
            
            if is_code_line:
                code_lines.append(line_stripped)
        
        if code_lines:
            return '\n'.join(code_lines)
        
        return text.strip()
    
    @staticmethod
    def clean_proof_code(code: str) -> str:
        """æ¸…æ´—è¯æ˜ä»£ç """
        # ç§»é™¤å¸¸è§çš„å›å¤å‰ç¼€
        prefixes = [
            r'Here is (?:the )?(?:proof|solution)[:\s]*',
            r'Proof[:\s]*',
            r'Solution[:\s]*',
            r'Here\'s (?:the )?(?:proof|solution)[:\s]*',
            r'Sure,? (?:here is|here\'s) (?:the )?(?:proof|solution)[:\s]*',
            r'Certainly[:\s]*',
            r'The (?:proof|solution) is[:\s]*',
        ]
        
        for prefix in prefixes:
            code = re.sub(prefix, '', code, flags=re.IGNORECASE)
        
        # ç§»é™¤å¸¸è§çš„ç»“å°¾æ ‡è®°
        suffixes = [
            r'\s*QED\.?\s*$',
            r'\s*âˆ\s*$',
            r'\s*This completes the proof\.?\s*$',
        ]
        
        for suffix in suffixes:
            code = re.sub(suffix, '', code, flags=re.IGNORECASE)
        
        return code.strip()
    
    @staticmethod
    def validate_lean_syntax(code: str) -> Tuple[bool, str]:
        """åŸºç¡€è¯­æ³•éªŒè¯"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®šç†å£°æ˜
        if not any(keyword in code for keyword in ['theorem', 'lemma', 'example']):
            return False, "No theorem/lemma/example declaration found"
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯æ˜ä½“
        if ':=' not in code and 'by' not in code and 'begin' not in code:
            return False, "No proof body found"
        
        # æ£€æŸ¥æ‹¬å·æ˜¯å¦å¹³è¡¡ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
        if code.count('(') != code.count(')'):
            return False, "Unbalanced parentheses"
        
        return True, ""

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = Config.CACHE_DIR):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache_file = self.cache_dir / "verification_cache.pkl"
        self.cache = self._load_cache()
    
    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except (pickle.PickleError, EOFError):
                logger.warning("Cache file corrupted, starting fresh")
                return {}
        return {}
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(self.cache) > Config.CACHE_MAX_SIZE:
                # ä¿ç•™æœ€è¿‘ä½¿ç”¨çš„æ¡ç›®
                items = sorted(self.cache.items(), key=lambda x: x[1].get('timestamp', 0))
                self.cache = dict(items[-Config.CACHE_MAX_SIZE:])
            
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.error(f"Failed to save cache: {e}")
    
    def get_cache_key(self, decl: str, proof: str) -> str:
        """è·å–ç¼“å­˜é”®"""
        normalized = CodeNormalizer.normalize_code(f"{decl} := {proof}")
        return hashlib.sha256(normalized.encode()).hexdigest()
    
    def get(self, cache_key: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if cache_key in self.cache:
            entry = self.cache[cache_key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
            if time.time() - entry.get('timestamp', 0) < 86400:
                return entry.get('result')
        return None
    
    def set(self, cache_key: str, result: Dict):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        self.cache[cache_key] = {
            'result': result,
            'timestamp': time.time()
        }
        # å®šæœŸä¿å­˜ç¼“å­˜
        if len(self.cache) % 100 == 0:
            self._save_cache()
    
    def save(self):
        """æ˜¾å¼ä¿å­˜ç¼“å­˜"""
        self._save_cache()

class ResourceMonitor:
    """èµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
        self.process = psutil.Process()
        # åˆå§‹åŒ–å†…å­˜
        try:
            self.start_memory = self.process.memory_info().rss
        except Exception:
            self.start_memory = 0
    
    def get_current_usage(self) -> Dict:
        """è·å–å½“å‰èµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            memory_info = self.process.memory_info()
            return {
                'memory_mb': memory_info.rss / 1024 / 1024,
                'cpu_percent': self.process.cpu_percent(interval=0.1),
                'threads': self.process.num_threads(),
                'elapsed_time': time.time() - self.start_time
            }
        except Exception:
            return {'memory_mb': 0, 'cpu_percent': 0, 'threads': 1, 'elapsed_time': 0}
    
    def check_system_limits(self) -> Tuple[bool, str]:
        """æ£€æŸ¥ç³»ç»Ÿé™åˆ¶"""
        try:
            # æ£€æŸ¥å¯ç”¨å†…å­˜
            mem = psutil.virtual_memory()
            if mem.available < Config.MAX_TOTAL_MEMORY_MB * 1024 * 1024:
                return False, f"Insufficient system memory: {mem.available / 1024 / 1024:.1f}MB available"
            
            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            if os.path.exists(Config.TEMP_DIR):
                disk = psutil.disk_usage(Config.TEMP_DIR)
                if disk.free < 1024 * 1024 * 1024:  # 1GB
                    return False, f"Insufficient disk space: {disk.free / 1024 / 1024:.1f}MB free"
            
            return True, "OK"
        except Exception as e:
            return False, f"Resource check failed: {e}"
    
    @staticmethod
    def get_system_info() -> Dict:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            return {
                'platform': platform.platform(),
                'processor': platform.processor(),
                'cpu_count': psutil.cpu_count(),
                'total_memory_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
                'python_version': platform.python_version(),
                'lean_version': ResourceMonitor._get_lean_version()
            }
        except Exception:
            return {}
    
    @staticmethod
    def _get_lean_version() -> Optional[str]:
        """è·å–Leanç‰ˆæœ¬"""
        try:
            result = subprocess.run(
                ['lean', '--version'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception:
            pass
        return None

# --- éªŒè¯å·¥ä½œè¿›ç¨‹ ---
def init_worker():
    """åˆå§‹åŒ–å·¥ä½œè¿›ç¨‹"""
    signal.signal(signal.SIGINT, signal.SIG_IGN)
    
    # è®¾ç½®è¿›ç¨‹åç§° (å¯é€‰ä¾èµ–)
    try:
        import setproctitle
        setproctitle.setproctitle(f"lean_verify_worker_{os.getpid()}")
    except ImportError:
        pass

def verify_single_proof(args: Tuple) -> Optional[VerificationResult]:
    """
    éªŒè¯å•ä¸ªè¯æ˜ï¼ˆåœ¨å­è¿›ç¨‹ä¸­è¿è¡Œï¼‰
    """
    code_snippet, original_decl, task_id, allow_sorry, timeout = args
    
    start_time = time.time()
    tmp_path = ""
    process_id = os.getpid()
    resource_monitor = ResourceMonitor()
    
    try:
        # === Level 1: é¢„éªŒè¯å’Œæ¸…æ´— ===
        
        # 1.1 åŸºç¡€æ¸…æ´—
        clean_code = CodeNormalizer.clean_proof_code(code_snippet)
        
        # 1.2 ä»Markdownæå–
        clean_code = CodeNormalizer.extract_code_from_markdown(clean_code)
        
        # 1.3 éç©ºæ£€æŸ¥
        if not clean_code or len(clean_code.strip()) < 5:
            return VerificationResult(
                task_id=task_id,
                original_decl=original_decl,
                solution=code_snippet,
                proof_only=clean_code,
                normalized_hash="",
                length=0,
                is_complete_proof=False,
                verification_time=time.time() - start_time,
                status=VerificationStatus.INVALID_FORMAT,
                error_message="Empty or too short proof"
            )
        
        # 1.4 è¯­æ³•éªŒè¯
        syntax_ok, syntax_error = CodeNormalizer.validate_lean_syntax(clean_code)
        if not syntax_ok:
            return VerificationResult(
                task_id=task_id,
                original_decl=original_decl,
                solution=code_snippet,
                proof_only=clean_code,
                normalized_hash="",
                length=len(clean_code),
                is_complete_proof=False,
                verification_time=time.time() - start_time,
                status=VerificationStatus.INVALID_FORMAT,
                error_message=syntax_error
            )
        
        # 1.5 æ£€æŸ¥Sorry
        if not allow_sorry and Config.STRICT_BAD_PATTERNS.search(clean_code):
            return VerificationResult(
                task_id=task_id,
                original_decl=original_decl,
                solution=code_snippet,
                proof_only=clean_code,
                normalized_hash="",
                length=len(clean_code),
                is_complete_proof=False,
                verification_time=time.time() - start_time,
                status=VerificationStatus.CONTAINS_SORRY,
                error_message="Proof contains sorry/admit"
            )
        
        # === Level 2: ä»£ç æ„å»º ===
        
        # 2.1 ç¡®å®šæ˜¯å¦éœ€è¦åŒ…è£…
        full_code = ""
        if "theorem" in clean_code and ":=" in clean_code:
            full_code = clean_code
        else:
            # æ¨¡å‹åªè¾“å‡ºäº†è¯æ˜ä½“ï¼Œéœ€è¦æ„å»ºå®Œæ•´å®šç†
            proof_body = clean_code.strip()
            
            # å¯å‘å¼æ·»åŠ  'by' æˆ– 'begin'
            if not (proof_body.startswith("by") or 
                    proof_body.startswith("begin") or
                    proof_body.startswith("exact") or
                    proof_body.startswith("apply") or
                    proof_body.startswith("refine")):
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨ begin ... end
                if ";" in proof_body or "\n" in proof_body:
                    proof_body = f"begin\n  {proof_body}\nend"
                else:
                    proof_body = f"by {proof_body}"
            
            full_code = f"{original_decl} := {proof_body}"
        
        # æœ€ç»ˆéªŒè¯ç»“æ„
        if ":=" not in full_code:
            return VerificationResult(
                task_id=task_id,
                original_decl=original_decl,
                solution=code_snippet,
                proof_only=clean_code,
                normalized_hash="",
                length=len(full_code),
                is_complete_proof=False,
                verification_time=time.time() - start_time,
                status=VerificationStatus.INVALID_FORMAT,
                error_message="Invalid proof structure"
            )
        
        # === Level 3: ç¼–è¯‘éªŒè¯ ===
        
        # 3.1 å‡†å¤‡æ–‡ä»¶å†…å®¹
        file_content = f"{Config.HEADER}{full_code}"
        
        # 3.2 ç”Ÿæˆä¸´æ—¶æ–‡ä»¶
        unique_id = f"{hashlib.md5(full_code.encode()).hexdigest()[:8]}_{process_id}_{uuid.uuid4().hex[:6]}"
        tmp_path = os.path.join(Config.TEMP_DIR, f"verify_{unique_id}.lean")
        
        with open(tmp_path, 'w', encoding='utf-8', errors='ignore') as f:
            f.write(file_content)
        
        # 3.3 ç¡®å®šè¶…æ—¶æ—¶é—´ï¼ˆæ ¹æ®è¯æ˜é•¿åº¦ï¼‰
        actual_timeout = timeout
        if len(full_code) > 1000:  # é•¿è¯æ˜ä½¿ç”¨æ›´é•¿è¶…æ—¶
            actual_timeout = min(timeout * 2, 300)
        
        # 3.4 ç¼–è¯‘å‘½ä»¤
        cmd = ["lake", "env", "lean", tmp_path]
        
        # 3.5 è®¾ç½®èµ„æºé™åˆ¶
        def preexec_fn():
            if sys.platform != "win32":
                try:
                    import resource
                    # è®¾ç½®å†…å­˜é™åˆ¶
                    soft, hard = resource.getrlimit(resource.RLIMIT_AS)
                    new_limit = Config.MAX_MEMORY_PER_WORKER_MB * 1024 * 1024
                    resource.setrlimit(resource.RLIMIT_AS, (new_limit, hard))
                    
                    # è®¾ç½®CPUæ—¶é—´é™åˆ¶
                    resource.setrlimit(resource.RLIMIT_CPU, (actual_timeout, actual_timeout + 10))
                except (ValueError, OSError, ImportError):
                    pass
        
        # 3.6 æ‰§è¡Œç¼–è¯‘
        start_compile = time.time()
        result = subprocess.run(
            cmd,
            cwd=Config.LEAN_GYM_PATH,
            capture_output=True,
            text=True,
            timeout=actual_timeout,
            encoding='utf-8',
            errors='ignore',
            preexec_fn=preexec_fn if sys.platform != "win32" else None
        )
        
        compile_time = time.time() - start_compile
        
        # 3.7 åˆ†æç»“æœ
        verification_time = time.time() - start_time
        memory_used = resource_monitor.get_current_usage()['memory_mb']
        
        # æ£€æŸ¥ç¼–è¯‘è¾“å‡º
        warnings = []
        if result.stderr:
            for line in result.stderr.split('\n'):
                if "warning" in line.lower():
                    warnings.append(line.strip())
        
        # 3.8 éªŒè¯æˆåŠŸæ¡ä»¶
        if result.returncode == 0:
            # æœ€ç»ˆæ£€æŸ¥sorryï¼ˆé˜²æ­¢ç¼–è¯‘å™¨è­¦å‘Šä½†é€šè¿‡ï¼‰
            has_sorry = bool(Config.STRICT_BAD_PATTERNS.search(full_code))
            
            if not allow_sorry and has_sorry:
                return VerificationResult(
                    task_id=task_id,
                    original_decl=original_decl,
                    solution=full_code,
                    proof_only=clean_code,
                    normalized_hash=hashlib.md5(CodeNormalizer.normalize_code(full_code).encode()).hexdigest(),
                    length=len(full_code),
                    is_complete_proof=False,
                    verification_time=verification_time,
                    status=VerificationStatus.CONTAINS_SORRY,
                    memory_used_mb=memory_used,
                    error_message="Proof contains sorry/admit"
                )
            
            # æˆåŠŸï¼
            return VerificationResult(
                task_id=task_id,
                original_decl=original_decl,
                solution=full_code,
                proof_only=clean_code,
                normalized_hash=hashlib.md5(CodeNormalizer.normalize_code(full_code).encode()).hexdigest(),
                length=len(full_code),
                is_complete_proof=not has_sorry,
                verification_time=verification_time,
                status=VerificationStatus.SUCCESS,
                memory_used_mb=memory_used,
                warnings=warnings
            )
        else:
            # ç¼–è¯‘å¤±è´¥
            error_msg = result.stderr[:500] if result.stderr else "Unknown compilation error"
            
            # ç¡®å®šé”™è¯¯ç±»å‹
            if "out of memory" in error_msg.lower():
                status = VerificationStatus.MEMORY_LIMIT
            elif "timeout" in error_msg.lower() or compile_time >= actual_timeout:
                status = VerificationStatus.TIMEOUT
            else:
                status = VerificationStatus.COMPILE_ERROR
            
            return VerificationResult(
                task_id=task_id,
                original_decl=original_decl,
                solution=full_code,
                proof_only=clean_code,
                normalized_hash=hashlib.md5(CodeNormalizer.normalize_code(full_code).encode()).hexdigest(),
                length=len(full_code),
                is_complete_proof=False,
                verification_time=verification_time,
                status=status,
                memory_used_mb=memory_used,
                error_message=error_msg,
                warnings=warnings
            )
            
    except subprocess.TimeoutExpired:
        return VerificationResult(
            task_id=task_id,
            original_decl=original_decl,
            solution=code_snippet,
            proof_only=clean_code,
            normalized_hash="",
            length=0,
            is_complete_proof=False,
            verification_time=time.time() - start_time,
            status=VerificationStatus.TIMEOUT,
            error_message=f"Timeout after {timeout} seconds"
        )
    except MemoryError:
        return VerificationResult(
            task_id=task_id,
            original_decl=original_decl,
            solution=code_snippet,
            proof_only=clean_code,
            normalized_hash="",
            length=0,
            is_complete_proof=False,
            verification_time=time.time() - start_time,
            status=VerificationStatus.MEMORY_LIMIT,
            error_message="Memory limit exceeded"
        )
    except Exception as e:
        return VerificationResult(
            task_id=task_id,
            original_decl=original_decl,
            solution=code_snippet,
            proof_only=clean_code,
            normalized_hash="",
            length=0,
            is_complete_proof=False,
            verification_time=time.time() - start_time,
            status=VerificationStatus.SYSTEM_ERROR,
            error_message=f"System error: {str(e)[:200]}"
        )
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if tmp_path and os.path.exists(tmp_path):
            try:
                os.remove(tmp_path)
            except OSError:
                pass
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        gc.collect()

# --- ä¸»è¦å¤„ç†é€»è¾‘ ---
class ProofVerifier:
    """è¯æ˜éªŒè¯å™¨ä¸»ç±»"""
    
    def __init__(self, args):
        self.args = args
        self.solved_tasks = defaultdict(list)
        self.cache_manager = CacheManager() if Config.ENABLE_CACHE else None
        self.resource_monitor = ResourceMonitor()
        self.stats = SystemStats()
        
        # åŠ è½½å·²æœ‰çš„ç»“æœç”¨äºå¢é‡å¤„ç†
        self.existing_results = {}
        if Config.ENABLE_INCREMENTAL and os.path.exists(self.args.output_file):
            self._load_existing_results()
    
    def _load_existing_results(self):
        """åŠ è½½å·²æœ‰ç»“æœ"""
        try:
            with open(self.args.output_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data = json.loads(line)
                        self.existing_results[data['task_id']] = data
            logger.info(f"ğŸ“‚ Loaded {len(self.existing_results)} existing results for incremental processing")
        except Exception as e:
            logger.warning(f"Failed to load existing results: {e}")
    
    def check_lean_environment(self) -> bool:
        """æ£€æŸ¥Leanç¯å¢ƒ"""
        logger.info("ğŸ” Checking Lean environment...")
        
        # æ£€æŸ¥lean_gymè·¯å¾„
        if not os.path.exists(Config.LEAN_GYM_PATH):
            logger.error(f"âŒ Lean gym path not found: {Config.LEAN_GYM_PATH}")
            return False
        
        # æ£€æŸ¥lakefile
        lakefile_lean = os.path.join(Config.LEAN_GYM_PATH, "lakefile.lean")
        lakefile_toml = os.path.join(Config.LEAN_GYM_PATH, "lakefile.toml")
        
        if not os.path.exists(lakefile_lean) and not os.path.exists(lakefile_toml):
            logger.error("âŒ No lakefile found in lean_gym directory")
            logger.error("ğŸ‘‰ Please run: git clone https://github.com/leanprover/lean4-simple.git lean_gym")
            return False
        
        # æµ‹è¯•lakeå‘½ä»¤
        try:
            # ç¼–è¯‘lean_gym
            logger.info("ğŸ› ï¸  Building lean_gym dependencies...")
            build_result = subprocess.run(
                ["lake", "build"],
                cwd=Config.LEAN_GYM_PATH,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if build_result.returncode != 0:
                logger.warning(f"âš ï¸  Lake build had issues: {build_result.stderr[:200]}")
                # ç»§ç»­å°è¯•ï¼Œå¯èƒ½éƒ¨åˆ†åŒ…å·²ç»æ„å»º
            
            # æµ‹è¯•ç®€å•è¯æ˜
            test_code = """
            theorem test : 1 + 1 = 2 := by
              norm_num
            """
            
            test_file = os.path.join(Config.TEMP_DIR, "test_env.lean")
            with open(test_file, 'w') as f:
                f.write("import Mathlib\n" + test_code)
            
            # æ³¨æ„ï¼šæ­¤å¤„å°†è¶…æ—¶æ—¶é—´å¢åŠ åˆ°äº† 600ç§’ (10åˆ†é’Ÿ)
            # å¹¶ä¸”æ•è·è¶…æ—¶å¼‚å¸¸ï¼Œå…è®¸è„šæœ¬ç»§ç»­è¿è¡Œ
            try:
                test_result = subprocess.run(
                    ["lake", "env", "lean", test_file],
                    cwd=Config.LEAN_GYM_PATH,
                    capture_output=True,
                    text=True,
                    timeout=600  # <--- ä¿®æ”¹: å¤§å¹…å¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                os.remove(test_file)
                
                if test_result.returncode == 0:
                    logger.info("âœ… Lean environment is ready!")
                    
                    # æ˜¾ç¤ºç³»ç»Ÿä¿¡æ¯
                    sys_info = self.resource_monitor.get_system_info()
                    logger.info(f"ğŸ“Š System info:")
                    logger.info(f"   Platform: {sys_info.get('platform', 'unknown')}")
                    logger.info(f"   CPU cores: {sys_info.get('cpu_count', 'unknown')}")
                    logger.info(f"   Total memory: {sys_info.get('total_memory_gb', 0):.1f} GB")
                    if sys_info.get('lean_version'):
                        logger.info(f"   Lean version: {sys_info['lean_version']}")
                    
                    return True
                else:
                    logger.error(f"âŒ Lean test failed: {test_result.stderr[:200]}")
                    return False
            except subprocess.TimeoutExpired:
                # ä¿®æ”¹: è¶…æ—¶ä¸å†ä½œä¸ºè‡´å‘½é”™è¯¯ï¼Œè€Œæ˜¯è­¦å‘Šå¹¶ç»§ç»­
                logger.warning("âš ï¸  Environment check timed out (likely due to slow I/O).")
                logger.warning("ğŸ‘‰ Proceeding anyway, as you have verified the environment manually.")
                if os.path.exists(test_file):
                    os.remove(test_file)
                return True
                
        except Exception as e:
            logger.error(f"âŒ Environment check failed: {e}")
            return False
    
    def load_tasks(self) -> List[Tuple]:
        """åŠ è½½å¾…éªŒè¯ä»»åŠ¡"""
        tasks = []
        skipped_lines = 0
        duplicate_tasks = 0
        
        logger.info(f"ğŸ“‚ Loading tasks from {self.args.input_file}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.args.input_file):
            logger.error(f"âŒ Input file not found: {self.args.input_file}")
            return []
        
        try:
            with open(self.args.input_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(tqdm(f, desc="Loading", unit="lines"), 1):
                    if not line.strip():
                        continue
                    
                    try:
                        data = json.loads(line)
                        task_id = data.get('task_id', f'line_{line_num}')
                        decl = data.get('original_decl', '').strip()
                        
                        # è·³è¿‡å·²æœ‰ç»“æœçš„ï¼ˆå¢é‡å¤„ç†ï¼‰
                        if task_id in self.existing_results and Config.ENABLE_INCREMENTAL:
                            duplicate_tasks += 1
                            continue
                        
                        # æå–è§£å†³æ–¹æ¡ˆ
                        solutions = []
                        
                        # æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
                        if 'solutions' in data and isinstance(data['solutions'], list):
                            solutions = data['solutions']
                        elif 'response' in data:
                            solutions = [data['response']]
                        elif 'solution' in data:
                            solutions = [data['solution']]
                        elif 'completion' in data:
                            solutions = [data['completion']]
                        
                        # å»é‡è§£å†³æ–¹æ¡ˆ
                        unique_solutions = set()
                        for sol in solutions:
                            if sol and isinstance(sol, str):
                                clean_sol = sol.strip()
                                if clean_sol and len(clean_sol) >= 5:  # æœ€å°é•¿åº¦
                                    unique_solutions.add(clean_sol)
                        
                        # æ·»åŠ ä»»åŠ¡
                        for sol in unique_solutions:
                            # æ£€æŸ¥ç¼“å­˜
                            cache_key = None
                            if self.cache_manager:
                                cache_key = self.cache_manager.get_cache_key(decl, sol)
                                cached_result = self.cache_manager.get(cache_key)
                                if cached_result:
                                    # ä½¿ç”¨ç¼“å­˜ç»“æœ
                                    if cached_result.get('status') == VerificationStatus.SUCCESS.value:
                                        if cached_result['task_id'] not in self.solved_tasks:
                                            self.solved_tasks[cached_result['task_id']] = []
                                        
                                        # è½¬æ¢å›å¯¹è±¡
                                        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…ä¸Šåº”è¯¥å®Œæ•´é‡å»º VerificationResult
                                        # ä½†å¯¹äºç»Ÿè®¡æ¥è¯´ï¼Œå­—å…¸å·²ç»è¶³å¤Ÿäº†
                                        # ä¸ºäº†ä¿æŒç±»å‹ä¸€è‡´ï¼Œè¿™é‡Œæˆ‘ä»¬åªåœ¨ç»Ÿè®¡æ—¶ä½¿ç”¨
                                        self.stats.successful_tasks += 1
                                        continue
                            
                            tasks.append((sol, decl, task_id, self.args.allow_sorry, self.args.timeout))
                            self.stats.total_tasks += 1
                            
                    except json.JSONDecodeError:
                        skipped_lines += 1
                    except Exception as e:
                        logger.debug(f"Error processing line {line_num}: {e}")
            
            logger.info(f"âœ… Loaded {len(tasks)} tasks from {line_num} lines")
            logger.info(f"   Skipped {skipped_lines} invalid JSON lines")
            logger.info(f"   Skipped {duplicate_tasks} already processed tasks")
            
            if not tasks:
                logger.warning("âš ï¸  No new tasks to process!")
            
            return tasks
            
        except Exception as e:
            logger.error(f"âŒ Failed to load tasks: {e}")
            traceback.print_exc()
            return []
    
    def verify_parallel(self, tasks: List[Tuple]):
        """å¹¶è¡ŒéªŒè¯æ‰€æœ‰ä»»åŠ¡"""
        if not tasks:
            logger.info("No tasks to verify")
            return
        
        logger.info(f"ğŸš€ Starting parallel verification with {self.args.num_workers} workers")
        logger.info(f"   Timeout per proof: {self.args.timeout}s")
        logger.info(f"   Memory limit per worker: {Config.MAX_MEMORY_PER_WORKER_MB}MB")
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        ok, msg = self.resource_monitor.check_system_limits()
        if not ok:
            logger.warning(f"âš ï¸  Resource warning: {msg}")
        
        try:
            # ä½¿ç”¨ProcessPoolExecutoræä¾›æ›´å¥½çš„æ§åˆ¶
            with ProcessPoolExecutor(
                max_workers=self.args.num_workers,
                initializer=init_worker,
                mp_context=multiprocessing.get_context('spawn' if sys.platform == "win32" else 'fork')
            ) as executor:
                
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_task = {
                    executor.submit(verify_single_proof, task): task 
                    for task in tasks
                }
                
                # å¤„ç†ç»“æœ
                with tqdm(total=len(tasks), desc="Verifying", unit="proofs") as pbar:
                    for future in as_completed(future_to_task):
                        try:
                            result = future.result(timeout=self.args.timeout + 5)
                            
                            if result:
                                # æ›´æ–°ç»Ÿè®¡
                                memory_used = result.memory_used_mb or 0
                                self.stats.update_stats(result, memory_used=memory_used)
                                
                                # ç¼“å­˜æˆåŠŸç»“æœ
                                if (result.status == VerificationStatus.SUCCESS and 
                                    self.cache_manager and 
                                    result.normalized_hash):
                                    
                                    cache_key = self.cache_manager.get_cache_key(
                                        result.original_decl, 
                                        result.proof_only
                                    )
                                    self.cache_manager.set(cache_key, result.to_dict())
                                
                                # å­˜å‚¨ç»“æœ
                                if result.status == VerificationStatus.SUCCESS:
                                    self.solved_tasks[result.task_id].append(result)
                            
                            pbar.update(1)
                            pbar.set_postfix({
                                'success': self.stats.successful_tasks,
                                'rate': f"{self.stats.get_summary()['success_rate']:.1%}"
                            })
                            
                        except Exception as e:
                            logger.error(f"Error processing future: {e}")
                            pbar.update(1)
                
                # ä¿å­˜ç¼“å­˜
                if self.cache_manager:
                    self.cache_manager.save()
        
        except KeyboardInterrupt:
            logger.warning("\nğŸ›‘ Verification interrupted by user")
            # ä¿å­˜å½“å‰è¿›åº¦
            if self.cache_manager:
                self.cache_manager.save()
            raise
        except Exception as e:
            logger.error(f"âŒ Parallel verification failed: {e}")
            traceback.print_exc()
            raise
    
    def select_best_solutions(self) -> List[Dict]:
        """é€‰æ‹©æœ€ä½³è§£å†³æ–¹æ¡ˆ"""
        final_data = []
        
        logger.info("ğŸ† Selecting best solutions...")
        
        for task_id, proofs in tqdm(self.solved_tasks.items(), desc="Selecting", unit="problems"):
            if not proofs:
                continue
            
            # 1. æŒ‰çŠ¶æ€å’Œå®Œæ•´æ€§åˆ†ç»„
            complete_proofs = []
            skeleton_proofs = []
            
            for proof in proofs:
                # å…¼å®¹ç¼“å­˜åŠ è½½çš„å­—å…¸ç±»å‹
                if isinstance(proof, dict):
                    status_val = proof.get('status')
                    is_complete = proof.get('is_complete_proof', False)
                    if status_val == VerificationStatus.SUCCESS.value:
                        if is_complete:
                            # ä¸´æ—¶åŒ…è£…æˆå¯¹è±¡ä»¥ä¾¿ç»Ÿä¸€å¤„ç†ï¼Œæˆ–è€…ä¿®æ”¹é€»è¾‘æ”¯æŒå­—å…¸
                            # è¿™é‡Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬å‡è®¾ proofs éƒ½æ˜¯å¯¹è±¡ï¼Œå¦‚æœæ··ç”¨äº†ç¼“å­˜ï¼Œéœ€è¦æ›´å¤æ‚çš„å¤„ç†
                            # é‰´äºä»£ç ç»“æ„ï¼Œæ–°è¿è¡Œçš„éªŒè¯éƒ½æ˜¯å¯¹è±¡ï¼Œåªæœ‰ä»load_tasksé‡Œæ¢å¤çš„ç¼“å­˜æ˜¯é—®é¢˜
                            # ç®€ä¾¿èµ·è§ï¼Œåªå¤„ç†å½“å‰è¿è¡Œäº§ç”Ÿçš„å¯¹è±¡ç»“æœ
                            pass
                else:
                    if proof.status == VerificationStatus.SUCCESS:
                        if proof.is_complete_proof:
                            complete_proofs.append(proof)
                        else:
                            skeleton_proofs.append(proof)
            
            # 2. é€‰æ‹©å€™é€‰åˆ—è¡¨
            candidates = []
            if complete_proofs:
                candidates = complete_proofs
            elif self.args.allow_sorry and skeleton_proofs:
                candidates = skeleton_proofs
            
            if not candidates:
                continue
            
            # 3. å»é‡å’Œæ’åº
            unique_candidates = self._deduplicate_candidates(candidates)
            
            if not unique_candidates:
                continue
            
            # 4. é€‰æ‹©æœ€ä½³
            best_solution = self._select_best_candidate(unique_candidates)
            
            # 5. æ·»åŠ åˆ°æœ€ç»ˆç»“æœ
            final_data.append(best_solution)
        
        logger.info(f"âœ… Selected {len(final_data)} best solutions")
        return final_data
    
    def _deduplicate_candidates(self, candidates: List[VerificationResult]) -> List[VerificationResult]:
        """å»é‡å€™é€‰è¯æ˜"""
        unique_map = {}
        
        for candidate in candidates:
            norm_hash = candidate.normalized_hash
            
            if norm_hash not in unique_map:
                unique_map[norm_hash] = candidate
            else:
                # é€‰æ‹©æ›´çŸ­æˆ–éªŒè¯æ—¶é—´æ›´çŸ­çš„
                existing = unique_map[norm_hash]
                if (candidate.length < existing.length or 
                    candidate.verification_time < existing.verification_time):
                    unique_map[norm_hash] = candidate
        
        return list(unique_map.values())
    
    def _select_best_candidate(self, candidates: List[VerificationResult]) -> Dict:
        """ä»å€™é€‰ä¸­é€‰æ‹©æœ€ä½³"""
        # æŒ‰å¤šä¸ªæ ‡å‡†æ’åº
        candidates.sort(key=lambda x: (
            x.length,                    # ä¼˜å…ˆæ›´çŸ­
            x.verification_time,         # å…¶æ¬¡æ›´å¿«
            -len(x.warnings)             # è­¦å‘Šå°‘çš„ä¼˜å…ˆ
        ))
        
        best = candidates[0]
        
        # è½¬æ¢ä¸ºå­—å…¸å¹¶æ·»åŠ å…ƒæ•°æ®
        result_dict = best.to_dict()
        result_dict['selection_metrics'] = {
            'total_candidates': len(candidates),
            'rank': 1,
            'selection_criteria': ['length', 'verification_time', 'warnings']
        }
        
        return result_dict
    
    def save_results(self, final_data: List[Dict]):
        """ä¿å­˜éªŒè¯ç»“æœ"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.dirname(self.args.output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # åˆå¹¶å¢é‡ç»“æœ
        if Config.ENABLE_INCREMENTAL and self.existing_results:
            all_data = {**self.existing_results}
            for item in final_data:
                all_data[item['task_id']] = item
            final_data = list(all_data.values())
        
        # ä¿å­˜ä¸»è¦ç»“æœ
        logger.info(f"ğŸ’¾ Saving results to {self.args.output_file}")
        
        with open(self.args.output_file, 'w', encoding='utf-8') as f:
            for item in final_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        # ä¿å­˜è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        stats_data = self._generate_statistics(final_data)
        
        stats_file = self.args.output_file.replace('.jsonl', '_stats.json')
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜é”™è¯¯åˆ†æ
        error_file = self.args.output_file.replace('.jsonl', '_errors.json')
        self._save_error_analysis(error_file)
        
        return stats_data
    
    def _generate_statistics(self, final_data: List[Dict]) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯"""
        complete_count = sum(1 for x in final_data 
                           if x.get('is_complete_proof', False))
        
        # æŒ‰çŠ¶æ€ç»Ÿè®¡
        status_counts = defaultdict(int)
        for item in final_data:
            status = item.get('status', 'unknown')
            status_counts[status] += 1
        
        # é•¿åº¦åˆ†å¸ƒ
        lengths = [item.get('length', 0) for item in final_data]
        
        stats = {
            "timestamp": datetime.now().isoformat(),
            "input_file": self.args.input_file,
            "output_file": self.args.output_file,
            "total_problems_processed": self.stats.total_tasks,
            "total_solutions_kept": len(final_data),
            "complete_proofs": complete_count,
            "skeleton_proofs": len(final_data) - complete_count,
            "status_distribution": dict(status_counts),
            "length_statistics": {
                "min": min(lengths) if lengths else 0,
                "max": max(lengths) if lengths else 0,
                "average": sum(lengths) / len(lengths) if lengths else 0,
                "median": sorted(lengths)[len(lengths)//2] if lengths else 0
            },
            "performance": self.stats.get_summary(),
            "system_info": self.resource_monitor.get_system_info(),
            "config": {
                "allow_sorry": self.args.allow_sorry,
                "num_workers": self.args.num_workers,
                "timeout": self.args.timeout,
                "max_memory_mb": Config.MAX_MEMORY_PER_WORKER_MB,
                "enable_cache": Config.ENABLE_CACHE,
                "enable_incremental": Config.ENABLE_INCREMENTAL
            }
        }
        
        return stats
    
    def _save_error_analysis(self, error_file: str):
        """ä¿å­˜é”™è¯¯åˆ†æ"""
        error_counts = defaultdict(int)
        
        for task_id, proofs in self.solved_tasks.items():
            for proof in proofs:
                # å…¼å®¹å­—å…¸å’Œå¯¹è±¡
                status = proof.get('status') if isinstance(proof, dict) else proof.status.value
                if status != VerificationStatus.SUCCESS.value:
                    error_counts[status] += 1
        
        analysis = {
            "error_distribution": dict(error_counts),
            "common_error_messages": self._extract_common_errors(),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(error_file, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
    
    def _extract_common_errors(self) -> List[Dict]:
        """æå–å¸¸è§é”™è¯¯"""
        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ›´è¯¦ç»†çš„é”™è¯¯åˆ†æ
        return []
    
    def run(self):
        """è¿è¡Œå®Œæ•´çš„éªŒè¯æµç¨‹"""
        # è®¾ç½®ä¸´æ—¶ç›®å½•
        logger.info(f"ğŸ“ Using temp directory: {Config.TEMP_DIR}")
        
        # æ£€æŸ¥ç¯å¢ƒ
        if not self.check_lean_environment():
            logger.error("âŒ Lean environment check failed")
            return False
        
        # åŠ è½½ä»»åŠ¡
        tasks = self.load_tasks()
        if not tasks:
            logger.warning("âš ï¸  No tasks to process")
            return False
        
        # éªŒè¯ä»»åŠ¡
        self.verify_parallel(tasks)
        
        # é€‰æ‹©æœ€ä½³è§£å†³æ–¹æ¡ˆ
        final_data = self.select_best_solutions()
        
        # ä¿å­˜ç»“æœ
        stats = self.save_results(final_data)
        
        # æ˜¾ç¤ºæœ€ç»ˆæŠ¥å‘Š
        self._print_final_report(stats)
        
        return True
    
    def _print_final_report(self, stats: Dict):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        logger.info("=" * 60)
        logger.info("ğŸ‰ VERIFICATION COMPLETE")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Summary:")
        logger.info(f"   Total problems processed: {stats['total_problems_processed']}")
        logger.info(f"   Solutions kept: {stats['total_solutions_kept']}")
        logger.info(f"   Complete proofs: {stats['complete_proofs']}")
        logger.info(f"   Skeleton proofs: {stats['skeleton_proofs']}")
        logger.info(f"   Success rate: {stats['performance']['success_rate']:.1%}")
        logger.info(f"   Average verification time: {stats['performance']['avg_time_per_task']:.2f}s")
        logger.info(f"   Total time: {stats['performance']['total_time_seconds']:.1f}s")
        logger.info(f"   Tasks per second: {stats['performance']['tasks_per_second']:.2f}")
        logger.info("")
        logger.info(f"ğŸ’¾ Results saved to: {self.args.output_file}")
        logger.info(f"ğŸ“ˆ Statistics saved to: {self.args.output_file.replace('.jsonl', '_stats.json')}")
        logger.info("=" * 60)

# --- ä¸»å‡½æ•° ---
def main():
    parser = argparse.ArgumentParser(
        description="ğŸš€ Lean 4 Proof Verifier - Advanced Parallel Verification System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python verify_proofs.py --input_file solutions.jsonl
  
  # Allow proofs with sorry
  python verify_proofs.py --allow_sorry --num_workers 8
  
  # Custom configuration
  python verify_proofs.py --timeout 60 --max_memory_mb 8192 --lean_gym_path /path/to/lean_gym
  
  # Disable cache and incremental processing
  python verify_proofs.py --disable-cache --disable-incremental
        """
    )
    
    # è¾“å…¥/è¾“å‡º
    parser.add_argument("--input_file", type=str, default=Config.DEFAULT_INPUT_FILE,
                        help="è¾“å…¥JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_file", type=str, default=Config.DEFAULT_OUTPUT_FILE,
                        help="è¾“å‡ºJSONLæ–‡ä»¶è·¯å¾„")
    
    # éªŒè¯é€‰é¡¹
    parser.add_argument("--allow_sorry", action="store_true",
                        help="å…è®¸åŒ…å«sorry/admitçš„è¯æ˜")
    parser.add_argument("--timeout", type=int, default=Config.TIMEOUT,
                        help="æ¯ä¸ªè¯æ˜çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    parser.add_argument("--max_memory_mb", type=int, default=Config.MAX_MEMORY_PER_WORKER_MB,
                        help="æ¯ä¸ªå·¥ä½œè¿›ç¨‹çš„æœ€å¤§å†…å­˜ï¼ˆMBï¼‰")
    
    # å¹¶è¡Œé€‰é¡¹
    parser.add_argument("--num_workers", type=int, default=Config.NUM_WORKERS,
                        help="å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°")
    
    # åŠŸèƒ½é€‰é¡¹
    parser.add_argument("--disable-cache", action="store_true",
                        help="ç¦ç”¨ç»“æœç¼“å­˜")
    parser.add_argument("--disable-incremental", action="store_true",
                        help="ç¦ç”¨å¢é‡å¤„ç†")
    
    # ç¯å¢ƒé€‰é¡¹
    parser.add_argument("--lean_gym_path", type=str, default=Config.LEAN_GYM_PATH,
                        help="lean_gymé¡¹ç›®è·¯å¾„")
    
    # æ—¥å¿—é€‰é¡¹
    parser.add_argument("--log_file", type=str,
                        help="æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šverification_YYYYMMDD_HHMMSS.logï¼‰")
    parser.add_argument("--debug", action="store_true",
                        help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    
    args = parser.parse_args()
    
    # åŠ¨æ€æ›´æ–°é…ç½®
    Config.update_from_args(args)
    
    # è®¾ç½®æ—¥å¿—
    if not args.log_file:
        args.log_file = f"verification_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    setup_logging(args.log_file)
    
    logger.info("=" * 60)
    logger.info("ğŸš€ Starting Lean 4 Proof Verifier")
    logger.info("=" * 60)
    logger.info(f"Input file: {args.input_file}")
    logger.info(f"Output file: {args.output_file}")
    logger.info(f"Workers: {args.num_workers}")
    logger.info(f"Timeout: {args.timeout}s")
    logger.info(f"Memory limit: {args.max_memory_mb}MB per worker")
    logger.info(f"Allow sorry: {args.allow_sorry}")
    logger.info(f"Cache enabled: {not args.disable_cache}")
    logger.info(f"Incremental processing: {not args.disable_incremental}")
    logger.info("=" * 60)
    
    # è¿è¡ŒéªŒè¯å™¨
    verifier = ProofVerifier(args)
    
    try:
        success = verifier.run()
        return 0 if success else 1
    except KeyboardInterrupt:
        logger.info("\n\nğŸ‘‹ Verification interrupted by user")
        return 130  # SIGINTé€€å‡ºç 
    except Exception as e:
        logger.error(f"\nâŒ Fatal error: {e}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    # å¤šè¿›ç¨‹æ”¯æŒ
    multiprocessing.freeze_support()
    
    if sys.platform == "win32":
        multiprocessing.set_start_method('spawn', force=True)
    
    # ç¡®ä¿ä¸´æ—¶ç›®å½•å­˜åœ¨
    Path(Config.TEMP_DIR).mkdir(parents=True, exist_ok=True)
    
    # è¿è¡Œä¸»ç¨‹åº
    sys.exit(main())